# -*- coding: utf-8 -*-
"""Lstm final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14JH3Rh0SlDzRUTTK8XUdkfXyLmsvVS2q
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from datetime import timedelta
from tqdm import tqdm

# Enable GPU acceleration
physical_devices = tf.config.experimental.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)

def apply_tukey_fences(df):
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df_clipped = df.clip(lower=lower_bound, upper=upper_bound, axis=1)
    return df_clipped

def run_model_on_dataset(data_path):
    df = pd.read_csv(data_path)

    # Apply Tukey fences to remove outliers
    df.iloc[:, 4:5] = apply_tukey_fences(df.iloc[:, 4:5])

    minmax = MinMaxScaler().fit(df.iloc[:, 4:5].astype('float32'))
    df_log = minmax.transform(df.iloc[:, 4:5].astype('float32'))
    df_log = pd.DataFrame(df_log)
    test_size = 30
    timestamp = 5
    dropout_rate = 0.8
    learning_rate = 0.01

    def forecast(df_train):
        model = tf.keras.Sequential([
            tf.keras.layers.LSTM(128, input_shape=(timestamp, df_log.shape[1]), return_sequences=True),
            tf.keras.layers.Dropout(dropout_rate),
            tf.keras.layers.LSTM(128),
            tf.keras.layers.Dense(df_log.shape[1])
        ])

        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='mse')

        X_train, y_train = [], []
        for i in range(0, len(df_train) - timestamp):
            X_train.append(df_train.iloc[i:i+timestamp].values)
            y_train.append(df_train.iloc[i+timestamp].values)
        X_train, y_train = np.array(X_train), np.array(y_train)

        model.fit(X_train, y_train, epochs=300, verbose=0)

        future_day = test_size
        output_predict = np.zeros((len(df_train) + future_day, df_train.shape[1]))
        output_predict[0] = df_train.iloc[0]

        for k in range(0, len(df_train) - timestamp, timestamp):
            out_logits = model.predict(np.expand_dims(df_train.iloc[k:k+timestamp], axis=0))
            output_predict[k + 1:k + timestamp + 1] = out_logits

        if len(df_train) % timestamp != 0:
            out_logits = model.predict(np.expand_dims(df_train.iloc[-timestamp:], axis=0))
            output_predict[len(df_train) - timestamp + 1:len(df_train) + 1] = out_logits
            future_day -= 1

        for i in range(future_day):
            o = output_predict[-future_day - timestamp + i:-future_day + i]
            out_logits = model.predict(np.expand_dims(o, axis=0))
            output_predict[-future_day + i] = out_logits[-1]

        output_predict = minmax.inverse_transform(output_predict)
        return output_predict[-test_size:]

    results = []
    for _ in tqdm(range(15), desc='Simulation'):
        df_train = df_log.iloc[:-test_size]
        results.append(forecast(df_train))

    accuracies = [calculate_accuracy(df['Close'].iloc[-test_size:].values, r) for r in results]

    plt.figure(figsize=(15, 5))
    for no, r in enumerate(results):
        plt.plot(r, label=f'Forecast {no + 1}')
    plt.plot(df['Close'].iloc[-test_size:].values, label='True trend', c='black')
    plt.legend()
    plt.title(f'Average accuracy: {np.median(accuracies):.4f} - {data_path.split(".")[0]}') # Add dataset name to the title
    plt.show()

    return accuracies

def calculate_accuracy(real, predict):
    real = np.array(real) + 1
    predict = np.array(predict) + 1
    percentage = 1 - np.sqrt(np.mean(np.square((real - predict) / real)))
    return percentage * 100

def main():
    data_paths = ["RELIANCE.NS.csv", "SBIN.NS.csv","BAJFINANCE.NS.csv","MARUTI.NS.csv","AXISBANK.NS.csv","ADANIPORTS.NS.csv","WIPRO.NS.csv","JSWSTEEL.NS.csv","TATASTEEL.NS.csv","DRREDDY.NS.csv"]  # Replace with actual paths (x50)
    accuracies = []
    for data_path in data_paths:
        dataset_accuracies = run_model_on_dataset(data_path)
        accuracies.extend(dataset_accuracies)

    average_accuracy = np.mean(accuracies)
    print("Average accuracy across datasets:", average_accuracy)

if __name__ == "__main__":
    main()